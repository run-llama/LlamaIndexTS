# @llamaindex/openai

## 0.1.17

### Patch Changes

- Updated dependencies [1364e8e]
- Updated dependencies [96fc69c]
  - @llamaindex/core@0.3.0

## 0.1.16

### Patch Changes

- 6a9a7b1: fix: take init api key into account

## 0.1.15

### Patch Changes

- Updated dependencies [5f67820]
  - @llamaindex/core@0.2.12

## 0.1.14

### Patch Changes

- Updated dependencies [ee697fb]
  - @llamaindex/core@0.2.11

## 0.1.13

### Patch Changes

- Updated dependencies [3489e7d]
- Updated dependencies [468bda5]
  - @llamaindex/core@0.2.10

## 0.1.12

### Patch Changes

- 2a82413: fix(core): set `Settings.llm` to OpenAI by default and support lazy load openai

## 0.1.11

### Patch Changes

- Updated dependencies [b17d439]
  - @llamaindex/core@0.2.9

## 0.1.10

### Patch Changes

- Updated dependencies [df441e2]
  - @llamaindex/core@0.2.8
  - @llamaindex/env@0.1.13

## 0.1.9

### Patch Changes

- 96f72ad: fix: openai streaming with token usage and finish_reason
- Updated dependencies [6cce3b1]
  - @llamaindex/core@0.2.7

## 0.1.8

### Patch Changes

- Updated dependencies [8b7fdba]
  - @llamaindex/core@0.2.6

## 0.1.7

### Patch Changes

- Updated dependencies [d902cc3]
  - @llamaindex/core@0.2.5

## 0.1.6

### Patch Changes

- Updated dependencies [b48bcc3]
  - @llamaindex/core@0.2.4
  - @llamaindex/env@0.1.12

## 0.1.5

### Patch Changes

- Updated dependencies [2cd1383]
  - @llamaindex/core@0.2.3

## 0.1.4

### Patch Changes

- Updated dependencies [749b43a]
  - @llamaindex/core@0.2.2

## 0.1.3

### Patch Changes

- Updated dependencies [ac07e3c]
- Updated dependencies [70ccb4a]
- Updated dependencies [1a6137b]
- Updated dependencies [ac07e3c]
  - @llamaindex/core@0.2.1
  - @llamaindex/env@0.1.11

## 0.1.2

### Patch Changes

- Updated dependencies [11feef8]
  - @llamaindex/core@0.2.0

## 0.1.1

### Patch Changes

- 7edeb1c: feat: decouple openai from `llamaindex` module

  This should be a non-breaking change, but just you can now only install `@llamaindex/openai` to reduce the bundle size in the future
