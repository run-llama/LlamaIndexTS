---
sidebar_position: 0
---

# Installation and Setup

We support Node.JS versions 18, 20 and 22, with experimental support for Deno, Bun and Vercel Edge functions.

## Installation from NPM

```bash npm2yarn
npm install llamaindex
```

### Environment variables

Our examples use OpenAI by default. You can use [other LLMs](../examples/other_llms) via their APIs; if you would prefer to use local models check out our [local LLM example](../examples/local_llm).

To use OpenAI, you'll need to [get an OpenAI API key](https://platform.openai.com/account/api-keys) and then make it available as an environment variable this way:

```bash
export OPENAI_API_KEY="sk-......" # Replace with your key
```

If you want to have it automatically loaded every time, add it to your `.zshrc/.bashrc`.

**WARNING:** do not check in your OpenAI key into version control. GitHub automatically invalidates OpenAI keys checked in by accident.

## What next?

- The easiest way to started is to [build a full-stack chat app with `create-llama`](starter_tutorial/chatbot).
- Try our other [getting started tutorials](starter_tutorial/retrieval_augmented_generation)
- Learn more about the [high level concepts](concepts) behind how LlamaIndex works
- Check out our [many examples](../examples/more_examples) of LlamaIndex.TS in action
