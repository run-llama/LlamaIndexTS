---
title: Using other LLM APIs
---

import { DynamicCodeBlock } from 'fumadocs-ui/components/dynamic-codeblock';
import CodeSource from "!raw-loader!../../../../../../../examples/mistral";

By default LlamaIndex.TS uses OpenAI's LLMs and embedding models, but we support [lots of other LLMs](../modules/llms) including models from Mistral (Mistral, Mixtral), Anthropic (Claude) and Google (Gemini).

If you don't want to use an API at all you can [run a local model](./local_llm).

This example runs you through the process of setting up a Mistral model:


## Installation

import { Tab, Tabs } from "fumadocs-ui/components/tabs";

<Tabs groupId="install" items={["npm", "yarn", "pnpm"]} persist>
	```shell tab="npm"
	npm install llamaindex @llamaindex/mistral
	```

	```shell tab="yarn"
	yarn add llamaindex @llamaindex/mistral
	```

	```shell tab="pnpm"
	pnpm add llamaindex @llamaindex/mistral
	```
</Tabs>

## Using another LLM

You can specify what LLM LlamaIndex.TS will use on the `Settings` object, like this:

```typescript
import { MistralAI } from "@llamaindex/mistral";
import { Settings } from "llamaindex";

Settings.llm = new MistralAI({
  model: "mistral-tiny",
  apiKey: "<YOUR_API_KEY>",
});
```

You can see examples of other APIs we support by checking out "Available LLMs" in the sidebar of our [LLMs section](../modules/llms).

## Using another embedding model

A frequent gotcha when trying to use a different API as your LLM is that LlamaIndex will also by default index and embed your data using OpenAI's embeddings. To completely switch away from OpenAI you will need to set your embedding model as well, for example:

```typescript
import { MistralAIEmbedding } from "@llamaindex/mistral";
import { Settings } from "llamaindex";

Settings.embedModel = new MistralAIEmbedding();
```

We support [many different embeddings](../modules/embeddings).

## Full example

This example uses Mistral's `mistral-tiny` model as the LLM and Mistral for embeddings as well.

<DynamicCodeBlock lang="ts" code={CodeSource} />
