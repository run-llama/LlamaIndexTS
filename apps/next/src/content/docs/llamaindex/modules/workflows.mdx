---
title: Workflows
---

import { DynamicCodeBlock } from 'fumadocs-ui/components/dynamic-codeblock';
import CodeSource from "!raw-loader!../../../../../../../examples/workflow/joke.ts";

A `Workflow` in LlamaIndexTS is an event-driven abstraction used to chain together several events. Workflows are made up of `steps`, with each step responsible for handling certain event types and emitting new events.

Workflows in LlamaIndexTS work by defining step functions that handle specific event types and emit new events.

When a step function is added to a workflow, you need to specify the input and optionally the output event types (used for validation). The specification of the input events ensures each step only runs when an accepted event is ready.

You can create a `Workflow` to do anything! Build an agent, a RAG flow, an extraction flow, or anything else you want.

import { Tab, Tabs } from "fumadocs-ui/components/tabs";

<Tabs groupId="install" items={["npm", "yarn", "pnpm"]} persist>
	```shell tab="npm"
	npm install @llamaindex/workflow
	```

	```shell tab="yarn"
	yarn add @llamaindex/workflow
	```

	```shell tab="pnpm"
	pnpm add @llamaindex/workflow
	```
</Tabs>

## Getting Started

As an illustrative example, let's consider a naive workflow where a joke is generated and then critiqued.

<DynamicCodeBlock lang="ts" code={CodeSource} />

There's a few moving pieces here, so let's go through this piece by piece.

### Defining Workflow Events

```typescript
export class JokeEvent extends WorkflowEvent<{ joke: string }> {}
```

Events are user-defined classes that extend `WorkflowEvent` and contain arbitrary data provided as template argument. In this case, our workflow relies on a single user-defined event, the `JokeEvent` with a `joke` attribute of type `string`.

### Setting up the Workflow Class

```typescript
const llm = new OpenAI();
...
const jokeFlow = new Workflow<unknown, string, string>();
```

Our workflow is implemented by initiating the `Workflow` class with three generic types: the context type (unknown in this case), input type (string), and output type (string). For simplicity, we created an `OpenAI` llm instance.

### Workflow Entry Points

```typescript
const generateJoke = async (_: unknown, ev: StartEvent<string>) => {
  const prompt = `Write your best joke about ${ev.data}.`;
  const response = await llm.complete({ prompt });
  return new JokeEvent({ joke: response.text });
};
```

Here, we come to the entry-point of our workflow. While events are user-defined, there are two special-case events, the `StartEvent` and the `StopEvent`. The `StartEvent<string>` signifies where to send the initial workflow input of type string.

To add this step to the workflow, we use the `addStep` method with an object specifying the input and output event types:

```typescript
jokeFlow.addStep(
  {
    inputs: [StartEvent<string>],
    outputs: [JokeEvent],
  },
  generateJoke
);
```

### Workflow Exit Points

```typescript
const critiqueJoke = async (_: unknown, ev: JokeEvent) => {
  const prompt = `Give a thorough critique of the following joke: ${ev.data.joke}`;
  const response = await llm.complete({ prompt });
  return new StopEvent(response.text);
};
```

Here, we have our second and last step in the workflow. We know it's the last step because the special `StopEvent` is returned. When the workflow encounters a returned `StopEvent`, it immediately stops the workflow and returns the result.

Add this step to the workflow:

```typescript
jokeFlow.addStep(
  {
    inputs: [JokeEvent],
    outputs: [StopEvent<string>],
  },
  critiqueJoke
);
```

### Running the Workflow

```typescript
const result = await jokeFlow.run("pirates");
console.log(result.data.result);
```

Lastly, we run the workflow. The `.run()` method is async, so we use await here to wait for the result.

## Working with Global Context/State

Optionally, you can choose to use global context between steps by specifying the context type when creating the workflow. For example, maybe multiple steps need to access shared state:

```typescript
import { HandlerContext } from "@llamaindex/workflow";

type MyContextData = {
  query: string;
  intermediateResults: any[];
}

const query = async (context: HandlerContext<MyContextData>, ev: MyEvent) => {
  // get the query from the context
  const query = context.data.query;
  // do something with context and event
  const val = ...
  // store in context
  context.data.intermediateResults.push(val);

  return new StopEvent({ result });
};
```

## Waiting for Multiple Events

The context does more than just hold data, it also provides utilities to buffer and wait for multiple events.

For example, you might have a step that waits for a query and retrieved nodes before synthesizing a response:

```typescript
const synthesize = async (context: Context, ev: QueryEvent | RetrieveEvent) => {
  const events = context.collectEvents(ev, [QueryEvent | RetrieveEvent]);
  if (!events) {
    return;
  }
  const prompt = events
    .map((event) => {
      if (event instanceof QueryEvent) {
        return `Answer this query using the context provided: ${event.data.query}`;
      } else if (event instanceof RetrieveEvent) {
        return `Context: ${event.data.context}`;
      }
      return "";
    })
    .join("\n");

  const response = await llm.complete({ prompt });
  return new StopEvent({ result: response.text });
};
```

Using `ctx.collectEvents()` we can buffer and wait for ALL expected events to arrive. This function will only return events (in the requested order) once all events have arrived.

## Manually Triggering Events

Normally, events are triggered by returning another event during a step. However, events can also be manually dispatched using the `ctx.sendEvent(event)` method within a workflow.

## Examples

You can find many useful examples of using workflows in the [examples folder](https://github.com/run-llama/LlamaIndexTS/blob/main/examples/workflow).
